{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先熟悉一下自注意力机制\n",
    "# Q K V，通过同一个X得到，序列长度不变[32,512]，单个词向量的维度多少？\n",
    "# Q（q1、q2、q3）和K（每一个，包括k1、k2、k3）相乘如何表示（Q点乘K的转置）？得到 q1对每个k的相关度\n",
    "# 相关度除以根号d_k，归一化（softmax）得到概率权重（a1、a2、a3）\n",
    "# 根据概率加权求和 a1 * q1 + a2 * q2 + a3 * q3，但是实际上会引入V，对v1、v2、v3求加权结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5328, 0.5225, 0.4589,  ..., 0.4284, 0.5289, 0.5732],\n",
      "         [0.5318, 0.5257, 0.4594,  ..., 0.4305, 0.5265, 0.5766],\n",
      "         [0.5328, 0.5228, 0.4590,  ..., 0.4286, 0.5286, 0.5736],\n",
      "         ...,\n",
      "         [0.5331, 0.5204, 0.4583,  ..., 0.4274, 0.5306, 0.5706],\n",
      "         [0.5333, 0.5184, 0.4578,  ..., 0.4268, 0.5321, 0.5686],\n",
      "         [0.5329, 0.5220, 0.4588,  ..., 0.4282, 0.5293, 0.5726]],\n",
      "\n",
      "        [[0.5327, 0.5229, 0.4590,  ..., 0.4286, 0.5285, 0.5737],\n",
      "         [0.5328, 0.5227, 0.4590,  ..., 0.4285, 0.5287, 0.5734],\n",
      "         [0.5316, 0.5265, 0.4595,  ..., 0.4310, 0.5260, 0.5774],\n",
      "         ...,\n",
      "         [0.5329, 0.5215, 0.4586,  ..., 0.4279, 0.5298, 0.5719],\n",
      "         [0.5329, 0.5218, 0.4587,  ..., 0.4281, 0.5295, 0.5723],\n",
      "         [0.5315, 0.5266, 0.4595,  ..., 0.4311, 0.5259, 0.5776]],\n",
      "\n",
      "        [[0.5320, 0.5251, 0.4593,  ..., 0.4301, 0.5269, 0.5760],\n",
      "         [0.5328, 0.5223, 0.4589,  ..., 0.4283, 0.5291, 0.5729],\n",
      "         [0.5333, 0.5176, 0.4577,  ..., 0.4267, 0.5327, 0.5679],\n",
      "         ...,\n",
      "         [0.5320, 0.5252, 0.4593,  ..., 0.4302, 0.5269, 0.5761],\n",
      "         [0.5325, 0.5238, 0.4592,  ..., 0.4292, 0.5279, 0.5746],\n",
      "         [0.5332, 0.5184, 0.4579,  ..., 0.4268, 0.5321, 0.5686]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5328, 0.5228, 0.4590,  ..., 0.4285, 0.5286, 0.5736],\n",
      "         [0.5332, 0.5191, 0.4580,  ..., 0.4270, 0.5315, 0.5692],\n",
      "         [0.5334, 0.5168, 0.4575,  ..., 0.4266, 0.5334, 0.5671],\n",
      "         ...,\n",
      "         [0.5332, 0.5186, 0.4579,  ..., 0.4269, 0.5319, 0.5688],\n",
      "         [0.5317, 0.5261, 0.4595,  ..., 0.4308, 0.5262, 0.5770],\n",
      "         [0.5329, 0.5215, 0.4587,  ..., 0.4279, 0.5297, 0.5719]],\n",
      "\n",
      "        [[0.5329, 0.5219, 0.4588,  ..., 0.4281, 0.5294, 0.5725],\n",
      "         [0.5327, 0.5231, 0.4591,  ..., 0.4287, 0.5284, 0.5739],\n",
      "         [0.5330, 0.5211, 0.4586,  ..., 0.4277, 0.5300, 0.5715],\n",
      "         ...,\n",
      "         [0.5328, 0.5225, 0.4589,  ..., 0.4284, 0.5289, 0.5732],\n",
      "         [0.5321, 0.5248, 0.4593,  ..., 0.4299, 0.5272, 0.5757],\n",
      "         [0.5333, 0.5177, 0.4577,  ..., 0.4267, 0.5326, 0.5680]],\n",
      "\n",
      "        [[0.5328, 0.5225, 0.4589,  ..., 0.4284, 0.5289, 0.5732],\n",
      "         [0.5331, 0.5203, 0.4583,  ..., 0.4274, 0.5307, 0.5706],\n",
      "         [0.5333, 0.5177, 0.4577,  ..., 0.4267, 0.5326, 0.5680],\n",
      "         ...,\n",
      "         [0.5337, 0.5140, 0.4565,  ..., 0.4267, 0.5357, 0.5646],\n",
      "         [0.5317, 0.5261, 0.4595,  ..., 0.4308, 0.5262, 0.5771],\n",
      "         [0.5330, 0.5213, 0.4586,  ..., 0.4278, 0.5299, 0.5716]]],\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 多头注意力机制的实现\n",
    "# 输入x [B,T,D]\n",
    "# 输出y [B,T,D] 序列元素蕴含了T长度序列中与不同元素之间的信息\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import  layer_norm,nn\n",
    "import math\n",
    "\n",
    "class MySelfAttention(nn.Module):\n",
    "    def __init__(self,latent_dim,num_head,dropout):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.query = nn.Linear(latent_dim,latent_dim, bias=False)\n",
    "        self.key = nn.Linear(latent_dim,latent_dim, bias=False)\n",
    "        self.value = nn.Linear(latent_dim,latent_dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(latent_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        X [B,T,D]\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        H = self.num_head\n",
    "\n",
    "        #1.计算QKV\n",
    "        # B,T,1,D\n",
    "        Q = self.query(self.norm(x)).unsqueeze(2)\n",
    "        K = self.key(self.norm(x)).unsqueeze(1)\n",
    "\n",
    "        #2.计算注意力参数\n",
    "        # B,T,H,D'\n",
    "        Q = Q.view(B,T,H,-1)\n",
    "        # B,H,T,D'\n",
    "        K = K.view(B,T,H,-1)\n",
    "        attention = torch.einsum('bnhd,bmhd->bnmh',Q,K) / math.sqrt(D//H)\n",
    "        weight = self.dropout(F.softmax(attention,dim=2))\n",
    "\n",
    "        #3.与V加权求和\n",
    "        V = self.value(self.norm(x)).unsqueeze(2)\n",
    "        V = V.view(B,T,H,-1)\n",
    "        # B,T,H,D'\n",
    "        y = torch.einsum('bnmh,bmhd->bndh',weight,V).reshape(B,T,D) # bndh可以不？\n",
    "        return y\n",
    "\n",
    "input_tensor = torch.randn(32, 100, 512)  # batch of 32, sequence length of 100, embedding size of 512\n",
    "# sa = MySelfAttention(512,8,0.1)\n",
    "# output_tensor = sa(input_tensor)\n",
    "# print(output_tensor)\n",
    "\n",
    "# gobal aveage pooling\n",
    "gap = nn.AvgPool1d(kernel_size=512)\n",
    "# gaap = nn.AdaptiveAvgPool1d(1)\n",
    "# adaptive avg pool\n",
    "# aap = nn.AdaptiveAvgPool1d(256)\n",
    "output_tensor = gap(input_tensor)\n",
    "# output_tensor_2 = gaap(input_tensor)\n",
    "\n",
    "fc1 = nn.Linear(1,512)\n",
    "relu = nn.ReLU()\n",
    "fc2 = nn.Linear(512,512)\n",
    "sigmoid = nn.Sigmoid()\n",
    "y = relu(fc1(output_tensor))\n",
    "y = sigmoid(fc2(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 位置编码\n",
    "### mask注意力机制，能否和运动序列结合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('humanmac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0de7d0f7b5299dcdb0959360e787f77cec8dab00cdfa591e2d799b18722c501e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
